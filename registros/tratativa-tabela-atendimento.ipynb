{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1178c16a",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "archive_name  = \"\"\n",
    "raw_bucket    = \"\"\n",
    "output_bucket = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c37458e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, lit, coalesce, concat, to_timestamp, when,\n",
    "    last, array, sort_array, size, element_at, expr, collect_list, date_format\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "import sys\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURAÇÃO DO SPARK\n",
    "# ==============================================================================\n",
    "# Configurações para acesso ao S3 e pacotes do Hadoop AWS.\n",
    "# A autenticação será feita via Instance Profile da EC2.\n",
    "conf = SparkConf()\n",
    "conf.set('spark.jars.packages', 'org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.901')\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'com.amazonaws.auth.InstanceProfileCredentialsProvider')\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"TratativaAtendimentoUPA\").getOrCreate()\n",
    "\n",
    "# ==============================================================================\n",
    "# DEFINIÇÃO DE ARQUIVOS E CAMINHOS\n",
    "# ==============================================================================\n",
    "# s3_prefixo = 's3a://bucket-raw-upa-connect-sofh/arquivos/'\n",
    "# s3_destino = 's3a://bucket-trusted-upa-connect-sofh/tabela_atendimento_tratada/'\n",
    "\n",
    "# # Lista dos arquivos CSV a serem processados.\n",
    "# arquivos_atendimento = [\n",
    "#     'ATENDIMENTOS_SUJOS_2025-10-13.csv',\n",
    "#     'ATENDIMENTOS_SUJOS_2025-10-12.csv',\n",
    "#     'ATENDIMENTOS_SUJOS_2025-10-11.csv',\n",
    "#     'ATENDIMENTOS_SUJOS_2025-10-10.csv',\n",
    "#     'ATENDIMENTOS_SUJOS_2025-10-09.csv'\n",
    "# ]\n",
    "\n",
    "# ==============================================================================\n",
    "# LEITURA E UNIFICAÇÃO DOS DADOS\n",
    "# ==============================================================================\n",
    "# Cria os caminhos completos para cada arquivo no S3.\n",
    "caminhos_atendimento = f's3a://{raw_bucket}/{archive_name}'\n",
    "\n",
    "# Lê os múltiplos arquivos CSV em um único DataFrame.\n",
    "df_bruto = spark.read \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', 'true') \\\n",
    "    .csv(caminhos_atendimento)\n",
    "\n",
    "# ==============================================================================\n",
    "# ETAPA 1: TRANSFORMAÇÕES INICIAIS (NOMES E COLUNAS)\n",
    "# ==============================================================================\n",
    "# 1.1: Renomeia todas as colunas para minúsculas.\n",
    "df_renomeado = df_bruto.toDF(*[c.lower() for c in df_bruto.columns])\n",
    "\n",
    "# 1.2: Renomeia a coluna 'fk_pessoa' para 'fk_paciente'.\n",
    "df_renomeado = df_renomeado.withColumnRenamed(\"fk_pessoa\", \"fk_paciente\")\n",
    "\n",
    "# 1.3: CRIA/ATUALIZA A COLUNA 'chegou' para ser um TIMESTAMP completo,\n",
    "# unindo 'data' com o primeiro horário disponível ('chegou' ou 'triagem_horario').\n",
    "df_com_timestamp = df_renomeado.withColumn(\n",
    "    \"chegou\",\n",
    "    # A função coalesce retorna o primeiro valor não nulo da lista.\n",
    "    coalesce(\n",
    "        # TENTATIVA 1: Tenta montar o timestamp com a coluna original 'chegou'.\n",
    "        # Se 'chegou' for nulo, esta expressão inteira se tornará nula.\n",
    "        to_timestamp(\n",
    "            concat(\n",
    "                date_format(col(\"data\"), \"yyyy-MM-dd\"), \n",
    "                lit(\" \"), \n",
    "                date_format(col(\"chegou\"), \"HH:mm:ss\") # Extrai apenas a hora para segurança\n",
    "            ),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        ),\n",
    "        # TENTATIVA 2: Se a tentativa 1 falhou (retornou nulo), usa o horário da triagem.\n",
    "        # Este é o valor de fallback.\n",
    "        to_timestamp(\n",
    "            concat(\n",
    "                date_format(col(\"data\"), \"yyyy-MM-dd\"), \n",
    "                lit(\" \"), \n",
    "                date_format(col(\"triagem_horario\"), \"HH:mm:ss\") # Usa a hora da triagem\n",
    "            ),\n",
    "            \"yyyy-MM-dd HH:mm:ss\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# 1.4: Corrige todas as colunas de horário para usar a data da coluna 'data'.\n",
    "# Isso evita que o Spark atribua a data atual a colunas que contêm apenas horas.\n",
    "# Foi usada a coluna 'data' para garantir que a data de todos os eventos seja a mesma\n",
    "# dentro de um mesmo atendimento.\n",
    "colunas_horario = [\"triagem_horario\", \"sala_de_espera\", \"consultorio_horario\", \"saida\"]\n",
    "df_horarios_corrigidos = df_com_timestamp\n",
    "for nome_coluna in colunas_horario:\n",
    "    df_horarios_corrigidos = df_horarios_corrigidos.withColumn(\n",
    "        nome_coluna,\n",
    "        when(col(nome_coluna).isNotNull(),\n",
    "            to_timestamp(\n",
    "                concat(\n",
    "                    date_format(col(\"data\"), \"yyyy-MM-dd\"), # Pega a data correta.\n",
    "                    lit(\" \"),\n",
    "                    date_format(col(nome_coluna), \"HH:mm:ss\") # Pega APENAS o horário da coluna.\n",
    "                ),\n",
    "                \"yyyy-MM-dd HH:mm:ss\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# 1.5: Ordena o DataFrame pela coluna 'chegou' (agora com timestamp completo)\n",
    "# para garantir a ordem correta dos eventos, essencial para as funções de janela.\n",
    "df_ordenado = df_horarios_corrigidos.orderBy(\"chegou\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# ETAPA 2: TRATATIVA DE VALORES INVÁLIDOS NA COLUNA 'fk_upa'\n",
    "# ==============================================================================\n",
    "# Define uma janela para buscar o último valor válido anterior, USANDO 'chegou'.\n",
    "window_ffill = Window.orderBy(\"chegou\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "# 2.1: Cria uma coluna temporária que é nula se 'fk_upa' estiver fora do intervalo [1, 34].\n",
    "df_com_upa_valida = df_ordenado.withColumn(\n",
    "    \"upa_valida\",\n",
    "    when(col(\"fk_upa\").between(1, 34), col(\"fk_upa\"))\n",
    ")\n",
    "\n",
    "# 2.2: Usa a função 'last' para preencher os valores nulos com o último valor válido.\n",
    "df_com_ultimo_upa = df_com_upa_valida.withColumn(\n",
    "    \"ultima_upa_valida\",\n",
    "    last(\"upa_valida\", ignorenulls=True).over(window_ffill)\n",
    ")\n",
    "\n",
    "# 2.3: Atualiza a coluna 'fk_upa' original e remove as colunas temporárias.\n",
    "df_upa_tratada = df_com_ultimo_upa.withColumn(\n",
    "    \"fk_upa\",\n",
    "    coalesce(col(\"upa_valida\"), col(\"ultima_upa_valida\"))\n",
    ").drop(\"upa_valida\", \"ultima_upa_valida\")\n",
    "\n",
    "# ==============================================================================\n",
    "# ETAPA 3: TRATATIVA DE OUTLIERS E NULOS (TEMPERATURA E OXIMETRIA)\n",
    "# ==============================================================================\n",
    "# Define a ordenação de janelas para usar a coluna 'chegou'.\n",
    "\n",
    "# --- 3.1: Tratativa da Temperatura ---\n",
    "# Identifica valores válidos de temperatura, marcando outliers e nulos como null.\n",
    "df_temp_valida = df_upa_tratada.withColumn(\n",
    "    \"temp_valida\",\n",
    "    when((col(\"temperatura_paciente\") >= 35) & (col(\"temperatura_paciente\") <= 42), col(\"temperatura_paciente\"))\n",
    ")\n",
    "\n",
    "# Coleta os últimos 3 valores válidos em um array (baseado na ordem de 'chegou').\n",
    "df_com_array_temp = df_temp_valida.withColumn(\n",
    "    \"ultimas_3_temps\",\n",
    "    expr(\"slice(collect_list(temp_valida) OVER (ORDER BY chegou ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW), -3, 3)\")\n",
    ")\n",
    "\n",
    "\n",
    "# Calcula a mediana a partir do array, somente se o array não estiver vazio.\n",
    "df_com_mediana_temp = df_com_array_temp.withColumn(\n",
    "    \"mediana_temp\",\n",
    "    when(size(col(\"ultimas_3_temps\")) > 0,\n",
    "        element_at(\n",
    "            sort_array(col(\"ultimas_3_temps\")),\n",
    "            (size(col(\"ultimas_3_temps\")) / 2 + 0.5).cast(\"int\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Substitui os valores inválidos (outliers/nulos) pela mediana calculada.\n",
    "df_temp_tratada = df_com_mediana_temp.withColumn(\n",
    "    \"temperatura_paciente\",\n",
    "    coalesce(col(\"temp_valida\"), col(\"mediana_temp\"))\n",
    ")\n",
    "\n",
    "\n",
    "# --- 3.2: Tratativa da Oximetria ---\n",
    "# Identifica valores válidos de oximetria, marcando outliers e nulos como null.\n",
    "df_oxi_valida = df_temp_tratada.withColumn(\n",
    "    \"oxi_valida\",\n",
    "    when((col(\"oximetria_paciente\") >= 70) & (col(\"oximetria_paciente\") <= 100), col(\"oximetria_paciente\"))\n",
    ")\n",
    "\n",
    "# Coleta os últimos 3 valores válidos em um array (baseado na ordem de 'chegou').\n",
    "df_com_array_oxi = df_oxi_valida.withColumn(\n",
    "    \"ultimas_3_oxis\",\n",
    "    expr(\"slice(collect_list(oxi_valida) OVER (ORDER BY chegou ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW), -3, 3)\")\n",
    ")\n",
    "\n",
    "\n",
    "# Calcula a mediana a partir do array, somente se o array não estiver vazio.\n",
    "df_com_mediana_oxi = df_com_array_oxi.withColumn(\n",
    "    \"mediana_oxi\",\n",
    "    when(size(col(\"ultimas_3_oxis\")) > 0,\n",
    "        element_at(\n",
    "            sort_array(col(\"ultimas_3_oxis\")),\n",
    "            (size(col(\"ultimas_3_oxis\")) / 2 + 0.5).cast(\"int\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Substitui os valores inválidos (outliers/nulos) pela mediana calculada.\n",
    "df_final = df_com_mediana_oxi.withColumn(\n",
    "    \"oximetria_paciente\",\n",
    "    coalesce(col(\"oxi_valida\"), col(\"mediana_oxi\"))\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# ETAPA 4: LIMPEZA FINAL E SELEÇÃO DE COLUNAS\n",
    "# ==============================================================================\n",
    "\n",
    "# RENOVA O NOME DA COLUNA 'chegou' para 'data_hora'\n",
    "df_renomeado_final = df_final.withColumnRenamed(\"chegou\", \"data_hora\")\n",
    "\n",
    "# Define a ordem final e as colunas desejadas, removendo colunas temporárias\n",
    "# e a coluna 'data' (que foi combinada com 'chegou' no passo 1.3).\n",
    "colunas_finais = [\n",
    "    \"data_hora\", # AGORA É 'data_hora'\n",
    "    \"id_atendimento\",\n",
    "    \"fk_paciente\",\n",
    "    \"triagem_horario\",\n",
    "    \"triagem_sala\",\n",
    "    \"sala_de_espera\",\n",
    "    \"consultorio_horario\",\n",
    "    \"consultorio_sala\",\n",
    "    \"saida\",\n",
    "    \"temperatura_paciente\",\n",
    "    \"oximetria_paciente\",\n",
    "    \"fk_upa\"\n",
    "]\n",
    "\n",
    "# A coluna 'data' é implicitamente removida por não estar em colunas_finais\n",
    "# As colunas temporárias ultimas_3_temps, mediana_temp, etc. também são removidas.\n",
    "tabela_unificada = df_renomeado_final.select(colunas_finais)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf6632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL_OUTPUT_DIR = \"s3a://bucket-trusted-upa-connect-sofh/\"\n",
    "# FINAL_FILENAME = \"tabela_atendimentos_tratada.csv\"\n",
    "# TEMP_STAGING_DIR = f\"{FINAL_OUTPUT_DIR}/_temp_staging_integrated\"\n",
    "\n",
    "# # 1. Escreve o resultado no caminho temporário\n",
    "# print(f\"\\nEscrevendo dados temporariamente em: {TEMP_STAGING_DIR}\")\n",
    "\n",
    "# # NOTA: Coalesce(1) para garantir a geração de um único arquivo CSV.\n",
    "# tabela_unificada.coalesce(1).write \\\n",
    "#     .option('delimiter', ';') \\\n",
    "#     .option('header', 'true') \\\n",
    "#     .option('encoding', 'UTF-8') \\\n",
    "#     .mode('overwrite') \\\n",
    "#     .csv(TEMP_STAGING_DIR)\n",
    "\n",
    "# # 2. Renomeia o arquivo gerado\n",
    "# try:\n",
    "#     # Acessa a classe 'Path' da JVM através do gateway do Spark\n",
    "#     Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "    \n",
    "#     # Acessa a configuração do Hadoop\n",
    "#     hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    \n",
    "#     # Obtém o objeto FileSystem para o caminho temporário\n",
    "#     fs = Path(TEMP_STAGING_DIR).getFileSystem(hadoop_conf)\n",
    "\n",
    "#     # Encontra o arquivo gerado (part-00000-*.csv) dentro do diretório temporário\n",
    "#     list_status = fs.globStatus(Path(TEMP_STAGING_DIR + \"/part-00000-*.csv\"))\n",
    "\n",
    "#     if list_status:\n",
    "#         # Pega o caminho completo do arquivo gerado\n",
    "#         generated_file_path = list_status[0].getPath()\n",
    "\n",
    "#         # Define o caminho final e o nome específico para o arquivo\n",
    "#         final_output_path = Path(f\"{FINAL_OUTPUT_DIR}/{FINAL_FILENAME}\")\n",
    "\n",
    "#         # Renomeia (move) o arquivo para o caminho e nome definitivos\n",
    "#         fs.rename(generated_file_path, final_output_path)\n",
    "        \n",
    "#         # 3. Deleta o diretório temporário (que ficou vazio) e outros arquivos de metadados\n",
    "#         fs.delete(Path(TEMP_STAGING_DIR), True) \n",
    "        \n",
    "#         print(f\"\\n✅ Base integrada salva e renomeada com sucesso para: {final_output_path}\")\n",
    "\n",
    "#     else:\n",
    "#         print(\"\\nErro: Não foi possível encontrar o arquivo CSV gerado (part-00000-*.csv) no caminho temporário.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"\\nOcorreu um erro durante a renomeação do arquivo no S3: {e}\")\n",
    "\n",
    "# # Encerra a sessão Spark\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b682b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install boto3\n",
    "\n",
    "import boto3\n",
    "from io import StringIO, BytesIO\n",
    "from botocore.exceptions import ClientError\n",
    "import pandas as pd\n",
    "\n",
    "FINAL_OUTPUT_DIR = f\"s3a://{output_bucket}/\"\n",
    "FINAL_FILENAME = \"tabela_atendimentos_tratada.csv\"\n",
    "FINAL_FILE_PATH = f\"{FINAL_OUTPUT_DIR}{FINAL_FILENAME}\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "try:\n",
    "    # 🧾 Verifica se o arquivo existe\n",
    "    s3.head_object(Bucket=output_bucket, Key=FINAL_FILENAME)\n",
    "\n",
    "    print(\"📥 Arquivo existente encontrado. Lendo do S3...\")\n",
    "\n",
    "    # ⚙️ Lê o CSV existente direto do S3 (Spark)\n",
    "    df_existente = (\n",
    "        spark.read\n",
    "        .option('header', 'true')\n",
    "        .option('delimiter', ',')\n",
    "        .csv(f\"s3a://{output_bucket}/{FINAL_FILENAME}\")\n",
    "    )\n",
    "\n",
    "    # 🔗 Concatena (Spark -> Pandas)\n",
    "    df_existente_pd = df_existente.toPandas()\n",
    "    sensor_pd = tabela_unificada.toPandas()  # sensor_df também é Spark, então converte\n",
    "    df_final = pd.concat([df_existente_pd, sensor_pd], ignore_index=True)\n",
    "\n",
    "    print(\"✅ Arquivo existente atualizado com novos dados.\")\n",
    "\n",
    "except ClientError as e:\n",
    "    if e.response[\"Error\"][\"Code\"] == \"404\":\n",
    "        print(\"🚫 Arquivo não encontrado. Criando novo arquivo no bucket.\")\n",
    "        df_final = tabela_unificada.toPandas()\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# 📤 Salva no S3 via boto3\n",
    "csv_buffer = StringIO()\n",
    "df_final.to_csv(csv_buffer, index=False)\n",
    "\n",
    "s3.put_object(\n",
    "    Bucket=output_bucket,\n",
    "    Key=FINAL_FILENAME,\n",
    "    Body=csv_buffer.getvalue()\n",
    ")\n",
    "\n",
    "print(f\"✅ Arquivo '{FINAL_FILENAME}' salvo com sucesso no bucket '{output_bucket}'.\")\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
