{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0b5c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-50c6a22a-112e-4b08-ace9-f1efd16d5bc6;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 382ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 by [com.amazonaws#aws-java-sdk-bundle;1.12.262] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-50c6a22a-112e-4b08-ace9-f1efd16d5bc6\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/15ms)\n",
      "25/10/17 14:37:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a leitura e padronização dos DataFrames do bucket trusted...\n",
      "Lendo atendimentos de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Atendimentos/AtendimentosTratada.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/17 14:37:37 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo clima_tempo de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/ClimaTempo/ClimaTempoTratada.csv\n",
      "Lendo febre_amarela de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/FebreAmarela/FebreAmarelaTratada.csv\n",
      "Lendo sindrome_gripal de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/SindromeGripal/sindromeGripalTratada.csv\n",
      "Leitura e preparação concluídas.\n",
      "Chave de junção padronizada: DataRegistro\n",
      "\n",
      "Iniciando a junção dos DataFrames...\n",
      "Junção com clima_tempo concluída.\n",
      "Junção com febre_amarela concluída.\n",
      "Junção com sindrome_gripal concluída.\n",
      "Junção de todos os DataFrames concluída.\n",
      "root\n",
      " |-- DataRegistro: string (nullable = true)\n",
      " |-- ATENDIMENTOS_SEXO: string (nullable = true)\n",
      " |-- ATENDIMENTOS_TIPO_DE_UNIDADE: string (nullable = true)\n",
      " |-- ATENDIMENTOS_DESCRICAO_DO_PROCEDIMENTO: string (nullable = true)\n",
      " |-- ATENDIMENTOS_DESCRICAO_DO_CBO: string (nullable = true)\n",
      " |-- ATENDIMENTOS_NACIONALIDADE: string (nullable = true)\n",
      " |-- ATENDIMENTOS_TIPO_PROCEDIMENTO: string (nullable = true)\n",
      " |-- CLIMA_TEMPO_TEMPERATURE_2M: string (nullable = true)\n",
      " |-- CLIMA_TEMPO_RELATIVE_HUMIDITY_2M: string (nullable = true)\n",
      " |-- CLIMA_TEMPO_RAIN: string (nullable = true)\n",
      " |-- CLIMA_TEMPO_PRECIPITATION: string (nullable = true)\n",
      " |-- CLIMA_TEMPO_PM10: string (nullable = true)\n",
      " |-- CLIMA_TEMPO_PM2_5: string (nullable = true)\n",
      " |-- CLIMA_TEMPO_CARBON_MONOXIDE: string (nullable = true)\n",
      " |-- CLIMA_TEMPO_NITROGEN_DIOXIDE: string (nullable = true)\n",
      " |-- CLIMA_TEMPO_OZONE: string (nullable = true)\n",
      " |-- CLIMA_TEMPO_SULPHUR_DIOXIDE: string (nullable = true)\n",
      " |-- FEBRE_AMARELA_UF_LPI: string (nullable = true)\n",
      " |-- FEBRE_AMARELA_MUN_LPI: string (nullable = true)\n",
      " |-- FEBRE_AMARELA_SEXO: string (nullable = true)\n",
      " |-- FEBRE_AMARELA_IDADE: string (nullable = true)\n",
      " |-- FEBRE_AMARELA_ANO_IS: string (nullable = true)\n",
      " |-- FEBRE_AMARELA_OBITO: string (nullable = true)\n",
      " |-- FEBRE_AMARELA_DT_OBITO: string (nullable = true)\n",
      " |-- SINDROME_GRIPAL_id: string (nullable = true)\n",
      " |-- SINDROME_GRIPAL_idade: string (nullable = true)\n",
      " |-- SINDROME_GRIPAL_sexo: string (nullable = true)\n",
      " |-- SINDROME_GRIPAL_sintoma_individual: string (nullable = true)\n",
      " |-- SINDROME_GRIPAL_bairro: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas na base integrada: 1342365\n",
      "\n",
      "Escrevendo dados temporariamente em: s3a://bucket-client-upa-connect-sofh/basesExternas/_temp_staging_integrated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/17 14:38:07 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/10/17 14:38:16 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/10/17 14:38:17 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Base integrada salva e renomeada com sucesso para: s3a://bucket-client-upa-connect-sofh/basesExternas/BaseIntegrada.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format\n",
    "\n",
    "# =======================================================================\n",
    "# 1. CONFIGURAÇÃO E INICIALIZAÇÃO DO SPARK\n",
    "# =======================================================================\n",
    "conf = SparkConf()\n",
    "conf.set(\n",
    "    \"spark.jars.packages\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.901\"\n",
    ")\n",
    "conf.set(\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "    \"com.amazonaws.auth.InstanceProfileCredentialsProvider\"\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ExternalDataIntegration\").config(conf=conf).getOrCreate()\n",
    "\n",
    "# =======================================================================\n",
    "# 2. DEFINIÇÃO DOS CAMINHOS DOS ARQUIVOS DE ENTRADA E SAÍDA\n",
    "# =======================================================================\n",
    "\n",
    "# Arquivos de Entrada no bucket trusted\n",
    "INPUT_PATHS = {\n",
    "    # Arquivos com granularidade por hora/minuto (DATA_ISO)\n",
    "    \"atendimentos\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Atendimentos/AtendimentosTratada.csv\",\n",
    "    \"clima_tempo\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/ClimaTempo/ClimaTempoTratada.csv\",\n",
    "    \n",
    "    # Arquivos com granularidade diária\n",
    "    \"febre_amarela\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/FebreAmarela/FebreAmarelaTratada.csv\",\n",
    "    \"sindrome_gripal\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/SindromeGripal/sindromeGripalTratada.csv\"\n",
    "}\n",
    "\n",
    "# Caminho de Saída no bucket client\n",
    "FINAL_OUTPUT_DIR = \"s3a://bucket-client-upa-connect-sofh/basesExternas\"\n",
    "FINAL_FILENAME = \"BaseIntegrada.csv\"\n",
    "TEMP_STAGING_DIR = f\"{FINAL_OUTPUT_DIR}/_temp_staging_integrated\"\n",
    "\n",
    "# Coluna de Junção Comum (Padronizada para a Data apenas - YYYY-MM-DD)\n",
    "# ALTERADO: Nome da coluna de junção agora é 'DataRegistro'\n",
    "JOIN_COLUMN = \"DataRegistro\"\n",
    "\n",
    "# =======================================================================\n",
    "# 3. LEITURA DOS DADOS E PREPARAÇÃO PARA JOIN\n",
    "# =======================================================================\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "print(\"Iniciando a leitura e padronização dos DataFrames do bucket trusted...\")\n",
    "for name, path in INPUT_PATHS.items():\n",
    "    print(f\"Lendo {name} de: {path}\")\n",
    "    \n",
    "    # Leitura de CSV\n",
    "    df = spark.read.option(\"delimiter\", \";\") \\\n",
    "                   .option(\"header\", \"true\") \\\n",
    "                   .option(\"encoding\", \"UTF-8\") \\\n",
    "                   .csv(path)\n",
    "    \n",
    "    # Padronização da Coluna de Data\n",
    "    if name in [\"atendimentos\", \"clima_tempo\"]:\n",
    "        # Extrai apenas a data (YYYY-MM-DD) da coluna DATA_ISO (ISO 8601)\n",
    "        df = df.withColumn(\n",
    "            JOIN_COLUMN, \n",
    "            date_format(col(\"DATA_ISO\"), \"yyyy-MM-dd\")\n",
    "        ).drop(\"DATA_ISO\")\n",
    "        \n",
    "    elif name == \"febre_amarela\":\n",
    "        # Converte a coluna DT_IS (DD/MM/YYYY) para a data padronizada (YYYY-MM-DD)\n",
    "        df = df.withColumn(\n",
    "            JOIN_COLUMN, \n",
    "            date_format(to_date(col(\"DT_IS\"), \"dd/MM/yyyy\"), \"yyyy-MM-dd\")\n",
    "        ).drop(\"DT_IS\")\n",
    "        \n",
    "        # REMOÇÃO: Coluna FEBRE_AMARELA_ID será removida no bloco de remoção.\n",
    "        \n",
    "    elif name == \"sindrome_gripal\":\n",
    "        # Renomeia DATA_NOTIFICACAO_TRATADA para a coluna de junção padrão.\n",
    "        df = df.withColumnRenamed(\"DATA_NOTIFICACAO_TRATADA\", JOIN_COLUMN)\n",
    "        \n",
    "        # REMOÇÃO: Coluna SINDROME_GRIPAL_ID será removida no bloco de remoção.\n",
    "    \n",
    "    # Renomeia colunas para evitar conflitos (exceto a coluna de junção)\n",
    "    for c in df.columns:\n",
    "        if c.upper() != JOIN_COLUMN.upper():\n",
    "            df = df.withColumnRenamed(c, f\"{name.upper()}_{c}\")\n",
    "            \n",
    "    # Seleciona o DataFrame com a nova coluna de junção\n",
    "    dataframes[name] = df\n",
    "\n",
    "print(\"Leitura e preparação concluídas.\")\n",
    "print(f\"Chave de junção padronizada: {JOIN_COLUMN}\")\n",
    "\n",
    "# =======================================================================\n",
    "# 4. REALIZAÇÃO DA JUNÇÃO (FULL OUTER JOIN)\n",
    "# =======================================================================\n",
    "print(\"\\nIniciando a junção dos DataFrames...\")\n",
    "\n",
    "# Começa com o DataFrame de Atendimentos\n",
    "df_base = dataframes[\"atendimentos\"]\n",
    "\n",
    "# Realiza a junção com os demais DataFrames usando FULL OUTER JOIN para manter todos os registros de datas\n",
    "for name, df_join in dataframes.items():\n",
    "    if name != \"atendimentos\":\n",
    "        # Usamos full outer join para manter a maior granularidade possível (a data),\n",
    "        # garantindo que as datas não existentes em um lado sejam mantidas.\n",
    "        df_base = df_base.join(df_join, on=JOIN_COLUMN, how=\"full_outer\")\n",
    "        print(f\"Junção com {name} concluída.\")\n",
    "\n",
    "print(\"Junção de todos os DataFrames concluída.\")\n",
    "\n",
    "# =======================================================================\n",
    "# 5. REMOÇÃO DE COLUNAS DE ID E SELEÇÃO FINAL\n",
    "# =======================================================================\n",
    "\n",
    "# Colunas a serem removidas do resultado final\n",
    "COLUNAS_PARA_REMOVER = [\n",
    "    \"FEBRE_AMARELA_ID\",\n",
    "    \"SINDROME_GRIPAL_ID\"\n",
    "]\n",
    "\n",
    "# Cria uma lista de colunas para manter, excluindo as colunas de ID\n",
    "colunas_finais = [c for c in df_base.columns if c not in COLUNAS_PARA_REMOVER]\n",
    "\n",
    "# Aplica a seleção\n",
    "df_base = df_base.select(colunas_finais)\n",
    "\n",
    "df_base.printSchema()\n",
    "print(f\"Total de linhas na base integrada: {df_base.count()}\")\n",
    "\n",
    "# =======================================================================\n",
    "# 6. SALVANDO E RENOMEANDO O RESULTADO NO S3 (BUCKET CLIENT)\n",
    "# =======================================================================\n",
    "\n",
    "# 1. Escreve o resultado no caminho temporário\n",
    "print(f\"\\nEscrevendo dados temporariamente em: {TEMP_STAGING_DIR}\")\n",
    "\n",
    "# NOTA: Coalesce(1) para garantir a geração de um único arquivo CSV.\n",
    "df_base.coalesce(1).write \\\n",
    "    .option('delimiter', ';') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('encoding', 'UTF-8') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv(TEMP_STAGING_DIR)\n",
    "\n",
    "# 2. Renomeia o arquivo gerado\n",
    "try:\n",
    "    # Acessa a classe 'Path' da JVM através do gateway do Spark\n",
    "    Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "    \n",
    "    # Acessa a configuração do Hadoop\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    \n",
    "    # Obtém o objeto FileSystem para o caminho temporário\n",
    "    fs = Path(TEMP_STAGING_DIR).getFileSystem(hadoop_conf)\n",
    "\n",
    "    # Encontra o arquivo gerado (part-00000-*.csv) dentro do diretório temporário\n",
    "    list_status = fs.globStatus(Path(TEMP_STAGING_DIR + \"/part-00000-*.csv\"))\n",
    "\n",
    "    if list_status:\n",
    "        # Pega o caminho completo do arquivo gerado\n",
    "        generated_file_path = list_status[0].getPath()\n",
    "\n",
    "        # Define o caminho final e o nome específico para o arquivo\n",
    "        final_output_path = Path(f\"{FINAL_OUTPUT_DIR}/{FINAL_FILENAME}\")\n",
    "\n",
    "        # Renomeia (move) o arquivo para o caminho e nome definitivos\n",
    "        fs.rename(generated_file_path, final_output_path)\n",
    "        \n",
    "        # 3. Deleta o diretório temporário (que ficou vazio) e outros arquivos de metadados\n",
    "        fs.delete(Path(TEMP_STAGING_DIR), True) \n",
    "        \n",
    "        print(f\"\\n✅ Base integrada salva e renomeada com sucesso para: {final_output_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nErro: Não foi possível encontrar o arquivo CSV gerado (part-00000-*.csv) no caminho temporário.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcorreu um erro durante a renomeação do arquivo no S3: {e}\")\n",
    "\n",
    "# Encerra a sessão Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfc16bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8b7071a9-d202-476f-9f0b-fdab881ae3ca;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 555ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 by [com.amazonaws#aws-java-sdk-bundle;1.12.262] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8b7071a9-d202-476f-9f0b-fdab881ae3ca\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/11ms)\n",
      "25/10/25 22:49:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/25 22:49:32 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando a leitura e padronização dos DataFrames do bucket trusted...\n",
      "Lendo atendimentos de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Atendimentos/AtendimentosTratada.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 22:49:51 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo clima_tempo de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/ClimaTempo/ClimaTempoTratada.csv\n",
      "Lendo febre_amarela de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/FebreAmarela/FebreAmarelaTratada.csv\n",
      "Lendo sindrome_gripal de: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/SindromeGripal/sindromeGripalTratada.csv\n",
      "Leitura e preparação concluídas.\n",
      "Chave de junção padronizada: DataRegistro\n",
      "\n",
      "Iniciando a junção dos DataFrames...\n",
      "Junção com clima_tempo concluída.\n",
      "Junção com febre_amarela concluída.\n",
      "Junção com sindrome_gripal concluída.\n",
      "Junção de todos os DataFrames concluída.\n",
      "\n",
      "Convertendo colunas finais para snake_case...\n",
      "root\n",
      " |-- data_registro: string (nullable = true)\n",
      " |-- atendimentos_sexo: string (nullable = true)\n",
      " |-- atendimentos_tipo_de_unidade: string (nullable = true)\n",
      " |-- atendimentos_descricao_do_procedimento: string (nullable = true)\n",
      " |-- atendimentos_descricao_do_cbo: string (nullable = true)\n",
      " |-- atendimentos_nacionalidade: string (nullable = true)\n",
      " |-- atendimentos_tipo_procedimento: string (nullable = true)\n",
      " |-- clima_tempo_temperature_2_m: string (nullable = true)\n",
      " |-- clima_tempo_relative_humidity_2_m: string (nullable = true)\n",
      " |-- clima_tempo_rain: string (nullable = true)\n",
      " |-- clima_tempo_precipitation: string (nullable = true)\n",
      " |-- clima_tempo_pm10: string (nullable = true)\n",
      " |-- clima_tempo_pm2_5: string (nullable = true)\n",
      " |-- clima_tempo_carbon_monoxide: string (nullable = true)\n",
      " |-- clima_tempo_nitrogen_dioxide: string (nullable = true)\n",
      " |-- clima_tempo_ozone: string (nullable = true)\n",
      " |-- clima_tempo_sulphur_dioxide: string (nullable = true)\n",
      " |-- febre_amarela_uf_lpi: string (nullable = true)\n",
      " |-- febre_amarela_mun_lpi: string (nullable = true)\n",
      " |-- febre_amarela_sexo: string (nullable = true)\n",
      " |-- febre_amarela_idade: string (nullable = true)\n",
      " |-- febre_amarela_ano_is: string (nullable = true)\n",
      " |-- febre_amarela_obito: string (nullable = true)\n",
      " |-- febre_amarela_dt_obito: string (nullable = true)\n",
      " |-- sindrome_gripal_id: string (nullable = true)\n",
      " |-- sindrome_gripal_idade: string (nullable = true)\n",
      " |-- sindrome_gripal_sexo: string (nullable = true)\n",
      " |-- sindrome_gripal_sintoma_individual: string (nullable = true)\n",
      " |-- sindrome_gripal_bairro: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de linhas na base integrada: 1342365\n",
      "\n",
      "Escrevendo dados temporariamente em: s3a://bucket-client-upa-connect-sofh/basesExternas/_temp_staging_integrated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/25 22:50:22 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/10/25 22:53:23 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/10/25 22:53:24 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Base integrada salva e renomeada com sucesso para: s3a://bucket-client-upa-connect-sofh/basesExternas/BaseIntegrada.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, date_format\n",
    "import re  # <<< ADICIONADO: Importação para Regex\n",
    "\n",
    "# =======================================================================\n",
    "# 1. CONFIGURAÇÃO E INICIALIZAÇÃO DO SPARK\n",
    "# =======================================================================\n",
    "conf = SparkConf()\n",
    "conf.set(\n",
    "    \"spark.jars.packages\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.901\"\n",
    ")\n",
    "conf.set(\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "    \"com.amazonaws.auth.InstanceProfileCredentialsProvider\"\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ExternalDataIntegration\").config(conf=conf).getOrCreate()\n",
    "\n",
    "# --- INÍCIO: FUNÇÃO AUXILIAR SNAKE_CASE ---\n",
    "def to_snake_case(name):\n",
    "    \"\"\"\n",
    "    Converte uma string (CamelCase, UPPER_CASE, etc.) para snake_case.\n",
    "    Ex: \"DataRegistro\" -> \"data_registro\"\n",
    "    Ex: \"ATENDIMENTOS_SEXO\" -> \"atendimentos_sexo\"\n",
    "    Ex: \"CLIMA_TEMPO_PM2_5\" -> \"clima_tempo_pm2_5\"\n",
    "    \"\"\"\n",
    "    # Insere _ antes de letras maiúsculas (ex: DataRegistro -> Data_Registro)\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    # Insere _ antes de grupos de letras maiúsculas/números (ex: CBO, PM2_5)\n",
    "    s2 = re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "    # Converte tudo para minúsculo\n",
    "    return s2.lower()\n",
    "# --- FIM: FUNÇÃO AUXILIAR SNAKE_CASE ---\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 2. DEFINIÇÃO DOS CAMINHOS DOS ARQUIVOS DE ENTRADA E SAÍDA\n",
    "# =======================================================================\n",
    "\n",
    "# Arquivos de Entrada no bucket trusted\n",
    "INPUT_PATHS = {\n",
    "    # Arquivos com granularidade por hora/minuto (DATA_ISO)\n",
    "    \"atendimentos\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Atendimentos/AtendimentosTratada.csv\",\n",
    "    \"clima_tempo\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/ClimaTempo/ClimaTempoTratada.csv\",\n",
    "    \n",
    "    # Arquivos com granularidade diária\n",
    "    \"febre_amarela\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/FebreAmarela/FebreAmarelaTratada.csv\",\n",
    "    \"sindrome_gripal\": \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/SindromeGripal/sindromeGripalTratada.csv\"\n",
    "}\n",
    "\n",
    "# Caminho de Saída no bucket client\n",
    "FINAL_OUTPUT_DIR = \"s3a://bucket-client-upa-connect-sofh/basesExternas\"\n",
    "FINAL_FILENAME = \"BaseIntegrada.csv\"\n",
    "TEMP_STAGING_DIR = f\"{FINAL_OUTPUT_DIR}/_temp_staging_integrated\"\n",
    "\n",
    "# Coluna de Junção Comum (Padronizada para a Data apenas - YYYY-MM-DD)\n",
    "# ALTERADO: Nome da coluna de junção agora é 'DataRegistro'\n",
    "JOIN_COLUMN = \"DataRegistro\"\n",
    "\n",
    "# =======================================================================\n",
    "# 3. LEITURA DOS DADOS E PREPARAÇÃO PARA JOIN\n",
    "# =======================================================================\n",
    "\n",
    "dataframes = {}\n",
    "\n",
    "print(\"Iniciando a leitura e padronização dos DataFrames do bucket trusted...\")\n",
    "for name, path in INPUT_PATHS.items():\n",
    "    print(f\"Lendo {name} de: {path}\")\n",
    "    \n",
    "    # Leitura de CSV\n",
    "    df = spark.read.option(\"delimiter\", \";\") \\\n",
    "                   .option(\"header\", \"true\") \\\n",
    "                   .option(\"encoding\", \"UTF-8\") \\\n",
    "                   .csv(path)\n",
    "    \n",
    "    # Padronização da Coluna de Data\n",
    "    if name in [\"atendimentos\", \"clima_tempo\"]:\n",
    "        # Extrai apenas a data (YYYY-MM-DD) da coluna DATA_ISO (ISO 8601)\n",
    "        df = df.withColumn(\n",
    "            JOIN_COLUMN, \n",
    "            date_format(col(\"DATA_ISO\"), \"yyyy-MM-dd\")\n",
    "        ).drop(\"DATA_ISO\")\n",
    "        \n",
    "    elif name == \"febre_amarela\":\n",
    "        # Converte a coluna DT_IS (DD/MM/YYYY) para a data padronizada (YYYY-MM-DD)\n",
    "        df = df.withColumn(\n",
    "            JOIN_COLUMN, \n",
    "            date_format(to_date(col(\"DT_IS\"), \"dd/MM/yyyy\"), \"yyyy-MM-dd\")\n",
    "        ).drop(\"DT_IS\")\n",
    "        \n",
    "        # REMOÇÃO: Coluna FEBRE_AMARELA_ID será removida no bloco de remoção.\n",
    "        \n",
    "    elif name == \"sindrome_gripal\":\n",
    "        # Renomeia DATA_NOTIFICACAO_TRATADA para a coluna de junção padrão.\n",
    "        df = df.withColumnRenamed(\"DATA_NOTIFICACAO_TRATADA\", JOIN_COLUMN)\n",
    "        \n",
    "        # REMOÇÃO: Coluna SINDROME_GRIPAL_ID será removida no bloco de remoção.\n",
    "    \n",
    "    # Renomeia colunas para evitar conflitos (exceto a coluna de junção)\n",
    "    for c in df.columns:\n",
    "        if c.upper() != JOIN_COLUMN.upper():\n",
    "            df = df.withColumnRenamed(c, f\"{name.upper()}_{c}\")\n",
    "            \n",
    "    # Seleciona o DataFrame com a nova coluna de junção\n",
    "    dataframes[name] = df\n",
    "\n",
    "print(\"Leitura e preparação concluídas.\")\n",
    "print(f\"Chave de junção padronizada: {JOIN_COLUMN}\")\n",
    "\n",
    "# =======================================================================\n",
    "# 4. REALIZAÇÃO DA JUNÇÃO (FULL OUTER JOIN)\n",
    "# =======================================================================\n",
    "print(\"\\nIniciando a junção dos DataFrames...\")\n",
    "\n",
    "# Começa com o DataFrame de Atendimentos\n",
    "df_base = dataframes[\"atendimentos\"]\n",
    "\n",
    "# Realiza a junção com os demais DataFrames usando FULL OUTER JOIN para manter todos os registros de datas\n",
    "for name, df_join in dataframes.items():\n",
    "    if name != \"atendimentos\":\n",
    "        # Usamos full outer join para manter a maior granularidade possível (a data),\n",
    "        # garantindo que as datas não existentes em um lado sejam mantidas.\n",
    "        df_base = df_base.join(df_join, on=JOIN_COLUMN, how=\"full_outer\")\n",
    "        print(f\"Junção com {name} concluída.\")\n",
    "\n",
    "print(\"Junção de todos os DataFrames concluída.\")\n",
    "\n",
    "# =======================================================================\n",
    "# 5. REMOÇÃO DE COLUNAS DE ID, SELEÇÃO FINAL E RENOMEAÇÃO SNAKE_CASE\n",
    "# =======================================================================\n",
    "\n",
    "# Colunas a serem removidas do resultado final\n",
    "# ATENÇÃO: A lógica de prefixação (ex: \"SINDROME_GRIPAL_\") pode exigir\n",
    "# que esta lista seja atualizada para (ex: \"SINDROME_GRIPAL_SINDROME_GRIPAL_ID\").\n",
    "# O schema que você forneceu indica que as colunas na lista abaixo\n",
    "# podem não ser os nomes corretos após a prefixação.\n",
    "COLUNAS_PARA_REMOVER = [\n",
    "    \"FEBRE_AMARELA_ID\",\n",
    "    \"SINDROME_GRIPAL_ID\" \n",
    "    # Com base no seu schema, talvez devesse ser \"SINDROME_GRIPAL_id\"\n",
    "    # ou \"SINDROME_GRIPAL_SINDROME_GRIPAL_ID\"\n",
    "]\n",
    "\n",
    "# Cria uma lista de colunas para manter, excluindo as colunas de ID\n",
    "colunas_finais = [c for c in df_base.columns if c not in COLUNAS_PARA_REMOVER]\n",
    "\n",
    "# Aplica a seleção\n",
    "df_selecionado = df_base.select(colunas_finais)\n",
    "\n",
    "# --- INÍCIO: MODIFICAÇÃO PARA SNAKE_CASE ---\n",
    "print(\"\\nConvertendo colunas finais para snake_case...\")\n",
    "\n",
    "# Gera a lista de expressões 'select' com alias\n",
    "# Ex: [col(\"DataRegistro\").alias(\"data_registro\"), col(\"ATENDIMENTOS_SEXO\").alias(\"atendimentos_sexo\"), ...]\n",
    "select_exprs = [\n",
    "    col(c).alias(to_snake_case(c)) for c in df_selecionado.columns\n",
    "]\n",
    "\n",
    "# Aplica a renomeação final\n",
    "df_base = df_selecionado.select(select_exprs)\n",
    "# --- FIM: MODIFICAÇÃO PARA SNAKE_CASE ---\n",
    "\n",
    "\n",
    "df_base.printSchema()\n",
    "print(f\"Total de linhas na base integrada: {df_base.count()}\")\n",
    "\n",
    "# =======================================================================\n",
    "# 6. SALVANDO E RENOMEANDO O RESULTADO NO S3 (BUCKET CLIENT)\n",
    "# =======================================================================\n",
    "\n",
    "# 1. Escreve o resultado no caminho temporário\n",
    "print(f\"\\nEscrevendo dados temporariamente em: {TEMP_STAGING_DIR}\")\n",
    "\n",
    "# NOTA: Coalesce(1) para garantir a geração de um único arquivo CSV.\n",
    "df_base.coalesce(1).write \\\n",
    "    .option('delimiter', ';') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('encoding', 'UTF-8') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv(TEMP_STAGING_DIR)\n",
    "\n",
    "# 2. Renomeia o arquivo gerado\n",
    "try:\n",
    "    # Acessa a classe 'Path' da JVM através do gateway do Spark\n",
    "    Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "    \n",
    "    # Acessa a configuração do Hadoop\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    \n",
    "    # Obtém o objeto FileSystem para o caminho temporário\n",
    "    fs = Path(TEMP_STAGING_DIR).getFileSystem(hadoop_conf)\n",
    "\n",
    "    # Encontra o arquivo gerado (part-00000-*.csv) dentro do diretório temporário\n",
    "    list_status = fs.globStatus(Path(TEMP_STAGING_DIR + \"/part-00000-*.csv\"))\n",
    "\n",
    "    if list_status:\n",
    "        # Pega o caminho completo do arquivo gerado\n",
    "        generated_file_path = list_status[0].getPath()\n",
    "\n",
    "        # Define o caminho final e o nome específico para o arquivo\n",
    "        final_output_path = Path(f\"{FINAL_OUTPUT_DIR}/{FINAL_FILENAME}\")\n",
    "\n",
    "        # Renomeia (move) o arquivo para o caminho e nome definitivos\n",
    "        fs.rename(generated_file_path, final_output_path)\n",
    "        \n",
    "        # 3. Deleta o diretório temporário (que ficou vazio) e outros arquivos de metadados\n",
    "        fs.delete(Path(TEMP_STAGING_DIR), True) \n",
    "        \n",
    "        print(f\"\\n✅ Base integrada salva e renomeada com sucesso para: {final_output_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nErro: Não foi possível encontrar o arquivo CSV gerado (part-00000-*.csv) no caminho temporário.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcorreu um erro durante a renomeação do arquivo no S3: {e}\")\n",
    "\n",
    "# Encerra a sessão Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe61c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
