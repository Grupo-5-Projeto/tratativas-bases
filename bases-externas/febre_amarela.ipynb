{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bf243b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fe2477a7-bfd7-4144-939f-63ba6d980282;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 394ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 by [com.amazonaws#aws-java-sdk-bundle;1.12.262] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fe2477a7-bfd7-4144-939f-63ba6d980282\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/8ms)\n",
      "25/10/16 02:22:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n",
      "25/10/16 02:22:45 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs removidos na validação OBITO: [1253, 1358, 1397, 1411, 1476, 1522, 1826, 2212, 2303, 2304, 2404, 2478, 2600, 2608]\n",
      "Validação OBITO: 14 linhas removidas onde OBITO = SIM e DT_OBITO estava vazio ou nulo\n",
      "\n",
      "Escrevendo dados temporariamente em: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/FebreAmarela/_temp_staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/16 02:23:00 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/10/16 02:23:01 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dados salvos e renomeados com sucesso para: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/FebreAmarela/FebreAmarelaTratada.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, upper, translate, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# =======================================================================\n",
    "# 1. CONFIGURAÇÃO E INICIALIZAÇÃO DO SPARK\n",
    "# =======================================================================\n",
    "conf = SparkConf()\n",
    "conf.set(\n",
    "    \"spark.jars.packages\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.901\"\n",
    ")\n",
    "conf.set(\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "    \"com.amazonaws.auth.InstanceProfileCredentialsProvider\"\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "# Função para remover acentos de uma coluna\n",
    "def remover_acentos(df, nome_coluna):\n",
    "    acentos = \"áàâãäéèêëíìîïóòôõöúùûüçÁÀÂÃÄÉÈÊËÍÌÎÏÓÒÔÕÖÚÙÛÜÇ\"\n",
    "    sem_acentos = \"aaaaaeeeeiiiiooooouuuucAAAAAEEEEIIIIOOOOOUUUUC\"\n",
    "    return df.withColumn(nome_coluna, translate(col(nome_coluna), acentos, sem_acentos))\n",
    "\n",
    "# =======================================================================\n",
    "# 2. LEITURA DOS DADOS (ISO-8859-1)\n",
    "# =======================================================================\n",
    "df_raw = spark.read.option('delimiter', ';') \\\n",
    "                     .option('header', 'true') \\\n",
    "                     .option('nullValue', 'null') \\\n",
    "                     .option('encoding', 'ISO-8859-1') \\\n",
    "    .csv(\"s3a://bucket-raw-upa-connect-sofh/basesExternas/FebreAmarela/fa_casoshumanos_1994-2025.csv\")\n",
    "\n",
    "# =======================================================================\n",
    "# 3. SELEÇÃO E LIMPEZA\n",
    "# =======================================================================\n",
    "colunas_desejadas = ['ID', 'UF_LPI', 'MUN_LPI', 'SEXO', 'IDADE', 'DT_IS', 'ANO_IS', 'OBITO', 'DT_OBITO']\n",
    "df_limpo = df_raw.select(*colunas_desejadas)\n",
    "\n",
    "# Substitui strings vazias por null nas colunas numéricas e converte para Integer\n",
    "colunas_numericas = ['ID', 'IDADE', 'ANO_IS']\n",
    "for c in colunas_numericas:\n",
    "    df_limpo = df_limpo.withColumn(c, when(col(c) == \"\", None).otherwise(col(c)))\n",
    "    df_limpo = df_limpo.withColumn(c, col(c).cast(IntegerType()))\n",
    "\n",
    "# Remove linhas com nulos nas colunas críticas\n",
    "colunas_para_verificar = ['ID', 'UF_LPI', 'MUN_LPI', 'SEXO', 'IDADE', 'DT_IS', 'ANO_IS', 'OBITO']\n",
    "df_limpo = df_limpo.na.drop(subset=colunas_para_verificar)\n",
    "\n",
    "# Filtra apenas UF_LPI = SP\n",
    "df_limpo = df_limpo.filter(col(\"UF_LPI\") == \"SP\")\n",
    "\n",
    "# Converte colunas de texto para maiúsculas\n",
    "colunas_maiusculas = ['UF_LPI', 'MUN_LPI', 'SEXO', 'OBITO']\n",
    "for c in colunas_maiusculas:\n",
    "    df_limpo = df_limpo.withColumn(c, upper(col(c)))\n",
    "\n",
    "# =======================================================================\n",
    "# 4. VALIDAÇÃO: se OBITO = SIM, DT_OBITO deve estar preenchida\n",
    "# =======================================================================\n",
    "# Linhas que serão removidas\n",
    "linhas_invalidas = df_limpo.filter(\n",
    "    (col(\"OBITO\") == \"SIM\") & ((col(\"DT_OBITO\").isNull()) | (col(\"DT_OBITO\") == \"\"))\n",
    ")\n",
    "\n",
    "# Mostra os IDs dessas linhas\n",
    "# É necessário coletar antes de printar no ambiente Spark\n",
    "ids_invalidos = linhas_invalidas.select(\"ID\").rdd.flatMap(lambda x: x).collect()\n",
    "print(f\"IDs removidos na validação OBITO: {ids_invalidos}\")\n",
    "\n",
    "# Contagem para log\n",
    "linhas_removidas = linhas_invalidas.count()\n",
    "print(f\"Validação OBITO: {linhas_removidas} linhas removidas onde OBITO = SIM e DT_OBITO estava vazio ou nulo\")\n",
    "\n",
    "# Aplica o filtro para manter apenas linhas válidas\n",
    "df_limpo = df_limpo.filter(\n",
    "    (col(\"OBITO\") != \"SIM\") | ((col(\"OBITO\") == \"SIM\") & (col(\"DT_OBITO\").isNotNull()) & (col(\"DT_OBITO\") != \"\"))\n",
    ")\n",
    "\n",
    "\n",
    "colunas_com_acentos = ['MUN_LPI', 'OBITO']\n",
    "\n",
    "for c in colunas_com_acentos:\n",
    "    df_limpo = remover_acentos(df_limpo, c)\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 5. SALVANDO E RENOMEANDO O RESULTADO NO S3\n",
    "# =======================================================================\n",
    "\n",
    "# Define o caminho de saída definitivo no S3 (Diretório)\n",
    "FINAL_OUTPUT_DIR = \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/FebreAmarela\"\n",
    "\n",
    "# Define o nome do arquivo final\n",
    "FINAL_FILENAME = \"FebreAmarelaTratada.csv\"\n",
    "\n",
    "# Define o caminho temporário (subdiretório dentro da pasta principal)\n",
    "TEMP_STAGING_DIR = f\"{FINAL_OUTPUT_DIR}/_temp_staging\"\n",
    "\n",
    "# 1. Escreve o resultado no caminho temporário\n",
    "print(f\"\\nEscrevendo dados temporariamente em: {TEMP_STAGING_DIR}\")\n",
    "# Note que estamos usando df_limpo (seu DataFrame tratado)\n",
    "df_limpo.coalesce(1).write \\\n",
    "    .option('delimiter', ';') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('encoding', 'ISO-8859-1') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv(TEMP_STAGING_DIR)\n",
    "\n",
    "# 2. Renomeia o arquivo gerado\n",
    "try:\n",
    "    # Acessa a classe 'Path' da JVM através do gateway do Spark\n",
    "    Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "    \n",
    "    # Acessa a configuração do Hadoop\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    \n",
    "    # Obtém o objeto FileSystem para o caminho temporário\n",
    "    fs = Path(TEMP_STAGING_DIR).getFileSystem(hadoop_conf)\n",
    "\n",
    "    # Encontra o arquivo gerado (part-00000-*.csv) dentro do diretório temporário\n",
    "    list_status = fs.globStatus(Path(TEMP_STAGING_DIR + \"/part-00000-*.csv\"))\n",
    "\n",
    "    if list_status:\n",
    "        # Pega o caminho completo do arquivo gerado\n",
    "        generated_file_path = list_status[0].getPath()\n",
    "\n",
    "        # Define o caminho final e o nome específico para o arquivo\n",
    "        final_output_path = Path(f\"{FINAL_OUTPUT_DIR}/{FINAL_FILENAME}\")\n",
    "\n",
    "        # Renomeia (move) o arquivo para o caminho e nome definitivos\n",
    "        # Esta operação move o arquivo dentro do S3 e o renomeia.\n",
    "        fs.rename(generated_file_path, final_output_path)\n",
    "        \n",
    "        # 3. Deleta o diretório temporário (que ficou vazio) e outros arquivos de metadados\n",
    "        fs.delete(Path(TEMP_STAGING_DIR), True) \n",
    "        \n",
    "        print(f\"\\n✅ Dados salvos e renomeados com sucesso para: {final_output_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nErro: Não foi possível encontrar o arquivo CSV gerado (part-00000-*.csv) no caminho temporário.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcorreu um erro durante a renomeação do arquivo no S3: {e}\")\n",
    "\n",
    "\n",
    "# Encerra a sessão Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1608836d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd2891ff",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
