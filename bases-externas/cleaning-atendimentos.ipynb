{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28596916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.7/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-680acac1-d96b-4ff1-b60f-28f25f108316;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 362ms :: artifacts dl 18ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.901 by [com.amazonaws#aws-java-sdk-bundle;1.12.262] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-680acac1-d96b-4ff1-b60f-28f25f108316\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/17ms)\n",
      "25/10/17 13:32:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/context.py:317: FutureWarning: Python 3.7 support is deprecated in Spark 3.4.\n",
      "  warnings.warn(\"Python 3.7 support is deprecated in Spark 3.4.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo dados do S3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/17 13:32:07 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "/usr/local/lib/python3.7/site-packages/pyspark/sql/column.py:462: FutureWarning: A column as 'key' in getItem is deprecated as of Spark 3.0, and will not be supported in the future release. Use `column[key]` or `column.key` syntax instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schema Final Padronizado:\n",
      "root\n",
      " |-- SEXO: string (nullable = true)\n",
      " |-- TIPO_DE_UNIDADE: string (nullable = true)\n",
      " |-- DESCRICAO_DO_PROCEDIMENTO: string (nullable = true)\n",
      " |-- DESCRICAO_DO_CBO: string (nullable = true)\n",
      " |-- NACIONALIDADE: string (nullable = true)\n",
      " |-- TIPO_PROCEDIMENTO: string (nullable = true)\n",
      " |-- DATA_ISO: string (nullable = true)\n",
      "\n",
      "\n",
      "Escrevendo dados temporariamente em: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Atendimentos/_temp_staging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/17 13:32:19 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "25/10/17 13:32:20 WARN AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dados salvos e renomeados com sucesso para: s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Atendimentos/AtendimentosTratada.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, upper, translate, when, regexp_replace, trim, lit, create_map, to_timestamp, date_format\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "from itertools import chain\n",
    "import unicodedata\n",
    "\n",
    "# =======================================================================\n",
    "# 1. CONFIGURAÇÃO E INICIALIZAÇÃO DO SPARK\n",
    "# =======================================================================\n",
    "conf = SparkConf()\n",
    "conf.set(\n",
    "    \"spark.jars.packages\",\n",
    "    \"org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.11.901\"\n",
    ")\n",
    "conf.set(\n",
    "    \"spark.hadoop.fs.s3a.aws.credentials.provider\",\n",
    "    \"com.amazonaws.auth.InstanceProfileCredentialsProvider\"\n",
    ")\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataCleaningAtendimentosUPA\").config(conf=conf).getOrCreate()\n",
    "\n",
    "# =======================================================================\n",
    "# 2. FUNÇÕES DE LIMPEZA E PADRONIZAÇÃO\n",
    "# =======================================================================\n",
    "\n",
    "# Função auxiliar para padronizar nomes de colunas no Python\n",
    "def padronizar_coluna_py(col_name):\n",
    "    \"\"\"\n",
    "    Padroniza o nome da coluna: remove acentos, mantém alfanuméricos/espaços,\n",
    "    converte para maiúsculo e substitui espaços por '_'.\n",
    "    \"\"\"\n",
    "    # Normalização Unicode (melhor forma de remover acentos no Python)\n",
    "    normalized_name = ''.join(\n",
    "        c for c in unicodedata.normalize('NFKD', col_name)\n",
    "        if not unicodedata.combining(c)\n",
    "    )\n",
    "    # Remove caracteres especiais que não sejam letras, números ou espaço\n",
    "    cleaned_name = ''.join(c for c in normalized_name if c.isalnum() or c == ' ')\n",
    "    \n",
    "    # Converte para maiúsculo e substitui espaços por _\n",
    "    final_name = cleaned_name.upper().replace(' ', '_')\n",
    "    return final_name\n",
    "\n",
    "# Função para limpar e padronizar o CONTEÚDO de colunas string no PySpark\n",
    "# Simula a lógica NFKD + ascii ignore + regex replace\n",
    "def limpar_coluna_string_pyspark(col_name):\n",
    "    \"\"\"\n",
    "    Aplica a limpeza do conteúdo da coluna: maiúsculas, remoção de acentos comuns\n",
    "    e remoção de caracteres especiais, deixando apenas letras, números e espaços.\n",
    "    \"\"\"\n",
    "    col_expr = upper(col(col_name))\n",
    "    \n",
    "    # Remoção de acentos mais comuns usando translate\n",
    "    acentos = \"ÁÀÂÃÄÉÈÊËÍÌÎÏÓÒÔÕÖÚÙÛÜÇ\"\n",
    "    sem_acentos = \"AAAAAEEEEIIIIOOOOOUUUUC\"\n",
    "    col_expr = translate(col_expr, acentos, sem_acentos)\n",
    "    \n",
    "    # Remove caracteres que não são letras (A-Z), números (0-9) ou espaço\n",
    "    col_expr = regexp_replace(col_expr, r\"[^A-Z0-9 ]\", \"\")\n",
    "    \n",
    "    # Remove espaços extras no início e fim\n",
    "    col_expr = trim(col_expr)\n",
    "    \n",
    "    return col_expr\n",
    "\n",
    "# =======================================================================\n",
    "# 3. DEFINIÇÕES DE CAMINHO\n",
    "# =======================================================================\n",
    "INPUT_PATH = \"s3a://bucket-raw-upa-connect-sofh/basesExternas/Atendimentos/2025-06-06_Sistema_E-Saude_Medicos_-_Base_de_Dados.csv\"\n",
    "\n",
    "# Caminhos de destino\n",
    "FINAL_OUTPUT_DIR = \"s3a://bucket-trusted-upa-connect-sofh/BasesExternas/Atendimentos\"\n",
    "FINAL_FILENAME = \"AtendimentosTratada.csv\"\n",
    "TEMP_STAGING_DIR = f\"{FINAL_OUTPUT_DIR}/_temp_staging\"\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 4. LEITURA DOS DADOS\n",
    "# =======================================================================\n",
    "print(\"Lendo dados do S3...\")\n",
    "# Assumindo o encoding ISO-8859-1 (latin1) para a leitura inicial, \n",
    "# pois é comum em dados brasileiros e o script pandas sugeria isso.\n",
    "df_raw = spark.read.option('delimiter', ';') \\\n",
    "                     .option('header', 'true') \\\n",
    "                     .option('nullValue', 'null') \\\n",
    "                     .option('encoding', 'ISO-8859-1') \\\n",
    "    .csv(INPUT_PATH)\n",
    "\n",
    "# =======================================================================\n",
    "# 5. SELEÇÃO E FILTRO INICIAL\n",
    "# =======================================================================\n",
    "colunas_desejadas = [\"Data do Atendimento\", \"Sexo\", \"Tipo de Unidade\", \"Descrição do Procedimento\", \"Descrição do CBO\", \"Nacionalidade\"]\n",
    "df_limpo = df_raw.select(*colunas_desejadas)\n",
    "\n",
    "# Filtra apenas Tipo de Unidade = UPA\n",
    "df_final = df_limpo.filter(col(\"Tipo de Unidade\") == \"UPA\")\n",
    "\n",
    "# =======================================================================\n",
    "# 6. LIMPEZA E TRANSFORMAÇÃO DE COLUNAS\n",
    "# =======================================================================\n",
    "\n",
    "# 6.1. Limpeza de colunas string (Procedimento, Nacionalidade, CBO)\n",
    "colunas_para_limpar = [\"Descrição do Procedimento\", \"Nacionalidade\", \"Descrição do CBO\"]\n",
    "\n",
    "for c in colunas_para_limpar:\n",
    "    df_final = df_final.withColumn(c, limpar_coluna_string_pyspark(c))\n",
    "\n",
    "# 6.2. Mapeamento da Categoria do Procedimento\n",
    "mapa_categoria_dict = {\n",
    "    'ATENDIMENTO MEDICO EM UNIDADE DE PRONTO ATENDIMENTO': 'PRONTO ATENDIMENTO',\n",
    "    'ATENDIMENTO DE URGENCIA C OBSERVACAO ATE 24 HORAS EM ATENCAO ESPECIALIZADA': 'URGENCIA/OBSERVACAO',\n",
    "    'ACOLHIMENTO COM CLASSIFICACAO DE RISCO': 'ACOLHIMENTO'\n",
    "}\n",
    "\n",
    "# Cria um mapa PySpark para usar com a função 'when' ou 'select'\n",
    "mapping_expr = create_map([lit(x) for x in chain(*mapa_categoria_dict.items())])\n",
    "\n",
    "df_final = df_final.withColumn(\n",
    "    'Tipo Procedimento',\n",
    "    mapping_expr.getItem(col('Descrição do Procedimento'))\n",
    ")\n",
    "\n",
    "# 6.3. Conversão da Data para ISO 8601\n",
    "# Tentativa de inferir o formato da data original (pode ser \"DD/MM/YYYY HH:MM:SS\" ou similar)\n",
    "# Para datas mais complexas, usamos to_timestamp\n",
    "df_final = df_final.withColumn(\n",
    "    \"Data Iso\",\n",
    "    date_format(\n",
    "        to_timestamp(col(\"Data do Atendimento\"), \"dd/MM/yyyy HH:mm:ss\"), # Formato comum para CSV brasileiro\n",
    "        \"yyyy-MM-dd'T'HH:mm:ss\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 6.4. Drop da coluna original de data\n",
    "df_final = df_final.drop(\"Data do Atendimento\")\n",
    "\n",
    "# =======================================================================\n",
    "# 7. PADRONIZAÇÃO DOS NOMES DAS COLUNAS\n",
    "# =======================================================================\n",
    "\n",
    "# 7.1. Obtém os nomes atuais\n",
    "current_columns = df_final.columns\n",
    "\n",
    "# 7.2. Aplica a função de padronização aos nomes\n",
    "new_columns = [padronizar_coluna_py(c) for c in current_columns]\n",
    "\n",
    "# 7.3. Renomeia as colunas no DataFrame\n",
    "df_final = df_final.toDF(*new_columns)\n",
    "print(\"\\nSchema Final Padronizado:\")\n",
    "df_final.printSchema()\n",
    "\n",
    "\n",
    "# =======================================================================\n",
    "# 8. SALVANDO E RENOMEANDO O RESULTADO NO S3\n",
    "# =======================================================================\n",
    "\n",
    "# 1. Escreve o resultado no caminho temporário\n",
    "print(f\"\\nEscrevendo dados temporariamente em: {TEMP_STAGING_DIR}\")\n",
    "\n",
    "# Usamos o encoding 'UTF-8' na saída para o trusted, que é o padrão moderno\n",
    "df_final.coalesce(1).write \\\n",
    "    .option('delimiter', ';') \\\n",
    "    .option('header', 'true') \\\n",
    "    .option('encoding', 'UTF-8') \\\n",
    "    .mode('overwrite') \\\n",
    "    .csv(TEMP_STAGING_DIR)\n",
    "\n",
    "# 2. Renomeia o arquivo gerado\n",
    "try:\n",
    "    # Acessa a classe 'Path' da JVM através do gateway do Spark\n",
    "    Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "    \n",
    "    # Acessa a configuração do Hadoop\n",
    "    hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "    \n",
    "    # Obtém o objeto FileSystem para o caminho temporário\n",
    "    fs = Path(TEMP_STAGING_DIR).getFileSystem(hadoop_conf)\n",
    "\n",
    "    # Encontra o arquivo gerado (part-00000-*.csv) dentro do diretório temporário\n",
    "    list_status = fs.globStatus(Path(TEMP_STAGING_DIR + \"/part-00000-*.csv\"))\n",
    "\n",
    "    if list_status:\n",
    "        # Pega o caminho completo do arquivo gerado\n",
    "        generated_file_path = list_status[0].getPath()\n",
    "\n",
    "        # Define o caminho final e o nome específico para o arquivo\n",
    "        final_output_path = Path(f\"{FINAL_OUTPUT_DIR}/{FINAL_FILENAME}\")\n",
    "\n",
    "        # Renomeia (move) o arquivo para o caminho e nome definitivos\n",
    "        fs.rename(generated_file_path, final_output_path)\n",
    "        \n",
    "        # 3. Deleta o diretório temporário (que ficou vazio) e outros arquivos de metadados\n",
    "        fs.delete(Path(TEMP_STAGING_DIR), True) \n",
    "        \n",
    "        print(f\"\\n✅ Dados salvos e renomeados com sucesso para: {final_output_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nErro: Não foi possível encontrar o arquivo CSV gerado (part-00000-*.csv) no caminho temporário.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nOcorreu um erro durante a renomeação do arquivo no S3: {e}\")\n",
    "\n",
    "# Encerra a sessão Spark\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
